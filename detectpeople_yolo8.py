# -*- coding: utf-8 -*-
"""DetectPeople-yolo8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usFylMgPwBJyA9_OlRh5Zj-RCAXHLS7P
"""

!pip install ultralytics opencv-python-headless pycocotools

from google.colab import drive
drive.mount('/content/drive')

from ultralytics import YOLO

import matplotlib.pyplot as plt
import cv2
import json
from google.colab.patches import cv2_imshow
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
import numpy as np

# yolov8l.pt | yolov8m.pt
model = YOLO('yolov8n.pt')
model2 = YOLO('yolov8m.pt')
model3 = YOLO('yolov8l.pt')

def load_image(image_path):
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image_rgb

def detect_people(image, model):
    results = model(image, conf=0.25, iou=0.9)
    boxes = results[0].boxes.xyxy.cpu().numpy()
    classes = [model.names[int(cls)] for cls in results[0].boxes.cls.cpu().numpy()]
    return boxes, classes

def draw_boxes(image, boxes, classes):
    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = box[:4]
        label = classes[i]
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)
        cv2.putText(image, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
    return image

def display_image(image):
    if isinstance(image, np.ndarray) and image.dtype == np.uint8:
        plt.figure(figsize=(10, 10))
        plt.imshow(image)
        plt.axis('off')
        plt.show()
    else:
        raise TypeError("Gambar harus berupa array numpy dengan tipe data uint8")

def process_video(video_path, model, output_path='output_video.avi'):
    cap = cv2.VideoCapture(video_path)

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        boxes, classes = detect_people(frame_rgb, model)

        frame_with_boxes = draw_boxes(frame_rgb, boxes, classes)

        frame_bgr = cv2.cvtColor(frame_with_boxes, cv2.COLOR_RGB2BGR)

        out.write(frame_bgr)

    cap.release()
    out.release()

def detect_and_annotate(image_path, model, output_json='predictions.json'):
    image = load_image(image_path)

    boxes, classes = detect_people(image, model)

    predictions = []
    for i, box in enumerate(boxes):
        prediction = {
            "image_id": 1,
            "category_id": 1,
            "bbox": box[:4].tolist(),
            "score": 0.9  # Skor prediksi, ini harus digantikan dengan skor asli dari model jika tersedia
        }
        predictions.append(prediction)

    with open(output_json, 'w') as f:
        json.dump(predictions, f)

    image_with_boxes = draw_boxes(image.copy(), boxes, classes)
    display_image(image_with_boxes)

    return predictions

def display_video(video_path):
    video = open(video_path, "rb").read()
    video_b64 = b64encode(video).decode()

    video_tag = f'''
    <video width="640" height="480" controls>
        <source src="data:video/mp4;base64,{video_b64}" type="video/mp4">
    </video>
    '''

    return HTML(video_tag)

image_path = '/content/drive/MyDrive/datatest/walkpeople2.jpeg'
boxes, classes = detect_people(image, model)
image_with_boxes = draw_boxes(image.copy(), boxes, classes)
display_image(image_with_boxes)

image_path = '/content/drive/MyDrive/datatest/walkpeople2.jpeg'
image = load_image(image_path)
boxes, classes = detect_people(image, model2)
image_with_boxes = draw_boxes(image.copy(), boxes, classes)
#display_image(image_with_boxes)
detect_and_annotate(image_path, model2, output_json='/content/drive/MyDrive/datatest/predictions.json')

image_path = '/content/drive/MyDrive/datatest/walkpeople2.jpeg'
image = load_image(image_path)
boxes, classes = detect_people(image, model3)
image_with_boxes = draw_boxes(image.copy(), boxes, classes)
display_image(image_with_boxes)

def create_ground_truths():
    ground_truths = {
        "images": [
            {"id": 1, "width": 640, "height": 480, "file_name": "image1.jpg"},
        ],
        "annotations": [
            {"image_id": 1, "category_id": 1, "bbox": [50, 50, 100, 150], "area": 100*150, "iscrowd": 0, "id": 1},
        ],
        "categories": [
            {"id": 1, "name": "person", "supercategory": "none"},
        ]
    }
    with open('/content/ground_truths.json', 'w') as f:
        json.dump(ground_truths, f)

import json

def compute_map(predictions_path, ground_truths_path, iou_threshold=0.5):
    coco_gt = COCO(ground_truths_path)
    coco_dt = coco_gt.loadRes(predictions_path)

    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')
    coco_eval.params.iouThrs = [iou_threshold]

    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

    return coco_eval.stats[0]

# Hitung Map
map_score = compute_map('/content/predictions.json', '/content/ground_truths.json')
print(f"mAP: {map_score}")

video_path = '/content/drive/MyDrive/datatest/video1walkpeople.mp4'  # Update with the correct path to your video

output_path = '/content/drive/MyDrive/datatest/output_video.avi'
process_video(video_path, model, output_path=output_path)

# Tampilkan video yang dihasilkan
display_video(output_path)